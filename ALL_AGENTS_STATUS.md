# All Agents Status & Quick Reference

## Agent Overview

| Agent | Port | Status | Auto-Polling | Purpose |
|-------|------|--------|--------------|---------|
| **Backend** | 4000 | âœ… Ready | N/A | API Gateway, Authentication |
| **Frontend** | 5173 | âœ… Ready | Auto-refresh (5s) | User Interface |
| **MCP Server** | 8000 | âœ… Ready | N/A | Agent Orchestration |
| **Planner Agent** | 8001 | âœ… Ready | No | Parse user intent, create projects |
| **Dataset Agent** | 8002 | âœ… Ready | Yes (10s) | Search & download datasets |
| **Training Agent** | 8003 | âœ… Ready | Yes (10s) | Train & evaluate models |

## Status Flow

```
User Message
    â†“
Planner Agent (8001)
    â†“ Creates project
    status: pending_dataset
    â†“
Dataset Agent (8002) [Auto-polls every 10s]
    â†“ Downloads dataset
    status: pending_training
    â†“
Training Agent (8003) [Auto-polls every 10s]
    â†“ Trains model
    status: pending_evaluation
    â†“
Training Agent (8003) [Auto-polls every 10s]
    â†“ Evaluates model
    status: completed
    â†“
User downloads bundle
```

## Quick Start Commands

### Start All Services (Windows)

```bash
# Terminal 1 - Backend
cd backend
npm start

# Terminal 2 - Frontend
cd frontend
npm run dev

# Terminal 3 - MCP Server
cd mcp_server
python main.py

# Terminal 4 - Planner Agent
cd Planner-Agent/agent/planner
python main.py

# Terminal 5 - Dataset Agent
cd Dataset_Agent/agents/dataset
python main.py

# Terminal 6 - Training Agent
cd Trainer-Agent/agent
python main.py
```

### Or Use Batch Scripts

```bash
# Start each in separate terminal
start-backend.bat
start-frontend.bat
start-mcp-server.bat
start-planner-agent.bat
start-dataset-agent.bat
start-training-agent.bat
```

## Health Checks

```bash
# Backend
curl http://localhost:4000/health

# MCP Server
curl http://localhost:8000/health

# Planner Agent
curl http://localhost:8001/health

# Dataset Agent
curl http://localhost:8002/health

# Training Agent
curl http://localhost:8003/health
```

## Environment Files

### Backend (.env)
```bash
PORT=4000
MCP_SERVER_URL=http://localhost:8000
FIREBASE_PROJECT_ID=your-project-id
GOOGLE_APPLICATION_CREDENTIALS=path/to/firebase-key.json
```

### MCP Server (.env)
```bash
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_SERVICE_ROLE_KEY=your-key
GEMINI_API_KEY=your-key
PLANNER_AGENT_URL=http://127.0.0.1:8001
DATASET_AGENT_URL=http://127.0.0.1:8002
TRAINING_AGENT_URL=http://127.0.0.1:8003
```

### Planner Agent (.env)
```bash
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_KEY=your-key
GEMINI_API_KEY=your-key
```

### Dataset Agent (.env)
```bash
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_KEY=your-key
GCP_BUCKET_NAME=your-bucket
KAGGLE_USERNAME=your-username
KAGGLE_KEY=your-key
GOOGLE_APPLICATION_CREDENTIALS=path/to/gcp-key.json
```

### Training Agent (.env)
```bash
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_KEY=your-key
GCP_BUCKET_NAME=your-bucket
GOOGLE_APPLICATION_CREDENTIALS=path/to/gcp-key.json
LOG_LEVEL=INFO
BATCH_SIZE=64
DEFAULT_EPOCHS=10
DEFAULT_LEARNING_RATE=0.001
```

## Database Tables

### projects
- `id` (UUID) - Primary key
- `user_id` (UUID) - Foreign key to users
- `name` (TEXT) - Project name
- `description` (TEXT) - Project description
- `status` (TEXT) - Current status
- `task_type` (TEXT) - ML task type
- `framework` (TEXT) - ML framework
- `search_keywords` (TEXT[]) - Keywords for dataset search
- `metadata` (JSONB) - Additional metadata
- `created_at` (TIMESTAMP)
- `updated_at` (TIMESTAMP)

### datasets
- `id` (UUID) - Primary key
- `project_id` (UUID) - Foreign key to projects
- `name` (TEXT) - Dataset name
- `gcs_url` (TEXT) - GCP storage URL
- `size` (TEXT) - Dataset size
- `source` (TEXT) - Source (kaggle, etc.)
- `created_at` (TIMESTAMP)

### models
- `id` (UUID) - Primary key
- `project_id` (UUID) - Foreign key to projects
- `name` (TEXT) - Model name
- `framework` (TEXT) - Framework (pytorch)
- `gcs_url` (TEXT) - GCP storage URL
- `accuracy` (FLOAT) - Model accuracy
- `metadata` (JSONB) - Training metrics
- `created_at` (TIMESTAMP)

### agent_logs
- `id` (UUID) - Primary key
- `project_id` (UUID) - Foreign key to projects
- `agent_name` (TEXT) - Agent name
- `message` (TEXT) - Log message
- `log_level` (TEXT) - info/warning/error
- `created_at` (TIMESTAMP)

## Monitoring

### Check Database Status
```bash
python check-database-status.py
```

### Check Failed Projects
```bash
python check-failed-projects.py
```

### Fix Stuck Projects
```bash
python fix-stuck-projects.py
```

## Common Issues

### Port Already in Use

**Problem**: `Address already in use`

**Solution**:
```bash
# Windows - Find process using port
netstat -ano | findstr :8003

# Kill process
taskkill /PID <process_id> /F
```

### Agent Not Polling

**Problem**: Projects stuck in pending status

**Solution**:
1. Check if agent is running
2. Check health endpoint
3. Check polling status endpoint
4. Restart agent

### Database Connection Failed

**Problem**: `Connection refused` or `Authentication failed`

**Solution**:
1. Verify Supabase URL in `.env`
2. Verify Supabase key is service role key (not anon key)
3. Check internet connection
4. Check Supabase project is active

### GCP Upload Failed

**Problem**: `Failed to upload to GCP`

**Solution**:
1. Verify GCP credentials file exists
2. Verify bucket name is correct
3. Verify service account has Storage Admin role
4. Check bucket permissions

## Testing Workflow

### 1. Create Project

Frontend â†’ ML Chat:
```
Create a flower classification model with dataset not more than 2GB
```

### 2. Monitor Progress

Watch project card in frontend:
- Status badge updates automatically (every 5s)
- Agent pipeline shows progress
- Click "View Details" to see logs

### 3. Check Logs

```bash
# In Supabase
SELECT * FROM agent_logs 
WHERE project_id = 'your-project-id' 
ORDER BY created_at DESC;
```

### 4. Download Model

When status is `completed`:
- Click "Download Model" button
- Get bundle with model + predict.py + labels

## Performance Tips

### For Faster Development
- Reduce epochs: `DEFAULT_EPOCHS=5`
- Reduce batch size: `BATCH_SIZE=32`
- Use smaller model: `preferred_model=mobilenet_v2`

### For Better Accuracy
- Increase epochs: `DEFAULT_EPOCHS=20`
- Increase batch size: `BATCH_SIZE=128` (if GPU available)
- Use larger model: `preferred_model=resnet50`

### For Production
- Enable mixed precision training (GPU only)
- Use larger batch sizes
- Enable model compilation (PyTorch 2.0+)
- Monitor GPU usage with `nvidia-smi`

## Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend  â”‚ :5173
â”‚  (React)    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Backend   â”‚ :4000
â”‚  (Node.js)  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MCP Server  â”‚ :8000
â”‚  (Python)   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â”€â”€â”€â”€â”€â†’ Planner Agent :8001
       â”‚        (Parse intent)
       â”‚
       â”œâ”€â”€â”€â”€â”€â”€â†’ Dataset Agent :8002
       â”‚        (Download data)
       â”‚
       â””â”€â”€â”€â”€â”€â”€â†’ Training Agent :8003
                (Train & Evaluate)
                
All agents â†” Supabase (Database)
All agents â†” GCP Storage (Files)
```

## Summary

âœ… **All agents are configured and ready**
âœ… **Auto-polling enabled** for Dataset and Training agents
âœ… **Frontend auto-refresh** enabled (5s interval)
âœ… **Windows compatible** with proper temp paths
âœ… **GPU ready** (will use GPU if available)

Just start all services and create a project! ğŸš€
